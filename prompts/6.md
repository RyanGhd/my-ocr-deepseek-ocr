error
```
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[4], line 35
     26 tokenizer = AutoTokenizer.from_pretrained(
     27     model_path, 
     28     trust_remote_code=True,
     29     local_files_only=True
     30 )
     32 # Use 'eager' attention instead of 'flash_attention_2' for non-Ampere GPUs (T4, V100, etc.)
     33 # Options: 'eager' (standard), 'sdpa' (scaled dot product), 'flash_attention_2' (Ampere+ only)
     34 # Note: Using low_cpu_mem_usage=True to reduce peak memory during loading
---> 35 model = AutoModel.from_pretrained(
     36     model_path, 
     37     attn_implementation='eager', 
     38     trust_remote_code=True, 
     39     use_safetensors=True,
     40     local_files_only=True,
     41     torch_dtype=torch.bfloat16,
     42     low_cpu_mem_usage=True
     43 )
     45 model = model.eval().cuda().to(torch.bfloat16)
     47 # Clear cache after model loading

File /opt/micromamba/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:559, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    557     cls.register(config.__class__, model_class, exist_ok=True)
    558     model_class = add_generation_mixin_to_remote_model(model_class)
--> 559     return model_class.from_pretrained(
    560         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    561     )
    562 elif type(config) in cls._model_mapping.keys():
    563     model_class = _get_model_class(config, cls._model_mapping)

File /opt/micromamba/lib/python3.10/site-packages/transformers/modeling_utils.py:3577, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)
   3573         raise ValueError(
   3574             "DeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`."
   3575         )
   3576     elif not is_accelerate_available():
-> 3577         raise ImportError(
   3578             f"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
   3579         )
   3581 # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.
   3582 if load_in_4bit or load_in_8bit:

ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`

Click to add a cell.
```

I run main.py in a vertex instance which is built using the Dockerfile attached
this docker file has some packages installed 
when I run main.py, it throws the error above

solve the error
