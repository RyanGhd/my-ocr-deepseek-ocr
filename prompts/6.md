error
```
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /opt/models/deepseek-ocr and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/opt/micromamba/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
=====================
BASE:  torch.Size([1, 256, 1280])
PATCHES:  torch.Size([6, 100, 1280])
=====================
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[1], line 36
     33 image_file = 'images/1.png'
     34 output_path = 'images_output'
---> 36 res = model.infer(
     37     tokenizer, 
     38     prompt=prompt, 
     39     image_file=image_file, 
     40     output_path=output_path, 
     41     base_size=1024, 
     42     image_size=640, 
     43     crop_mode=True, 
     44     save_results=True, 
     45     test_compress=True
     46 )

File ~/.cache/huggingface/modules/transformers_modules/deepseek-ocr/modeling_deepseekocr.py:916, in DeepseekOCRForCausalLM.infer(self, tokenizer, prompt, image_file, output_path, base_size, image_size, crop_mode, test_compress, save_results, eval_mode)
    914     with torch.autocast("cuda", dtype=torch.bfloat16):
    915         with torch.no_grad():
--> 916             output_ids = self.generate(
    917                 input_ids.unsqueeze(0).cuda(),
    918                 images=[(images_crop.cuda(), images_ori.cuda())],
    919                 images_seq_mask = images_seq_mask.unsqueeze(0).cuda(),
    920                 images_spatial_crop = images_spatial_crop,
    921                 # do_sample=False,
    922                 # num_beams = 1,
    923                 temperature=0.0,
    924                 eos_token_id=tokenizer.eos_token_id,
    925                 streamer=streamer,
    926                 max_new_tokens=8192,
    927                 no_repeat_ngram_size = 20,
    928                 use_cache = True
    929                 )
    931 else:
    932     with torch.autocast("cuda", dtype=torch.bfloat16):

File /opt/micromamba/lib/python3.10/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)
    113 @functools.wraps(func)
    114 def decorate_context(*args, **kwargs):
    115     with ctx_factory():
--> 116         return func(*args, **kwargs)

File /opt/micromamba/lib/python3.10/site-packages/transformers/generation/utils.py:2215, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)
   2207     input_ids, model_kwargs = self._expand_inputs_for_generation(
   2208         input_ids=input_ids,
   2209         expand_size=generation_config.num_return_sequences,
   2210         is_encoder_decoder=self.config.is_encoder_decoder,
   2211         **model_kwargs,
   2212     )
   2214     # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)
-> 2215     result = self._sample(
   2216         input_ids,
   2217         logits_processor=prepared_logits_processor,
   2218         stopping_criteria=prepared_stopping_criteria,
   2219         generation_config=generation_config,
   2220         synced_gpus=synced_gpus,
   2221         streamer=streamer,
   2222         **model_kwargs,
   2223     )
   2225 elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):
   2226     # 11. prepare beam search scorer
   2227     beam_scorer = BeamSearchScorer(
   2228         batch_size=batch_size,
   2229         num_beams=generation_config.num_beams,
   (...)
   2234         max_length=generation_config.max_length,
   2235     )

File /opt/micromamba/lib/python3.10/site-packages/transformers/generation/utils.py:3206, in GenerationMixin._sample(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)
   3203 model_inputs.update({"output_hidden_states": output_hidden_states} if output_hidden_states else {})
   3205 # forward pass to get next token
-> 3206 outputs = self(**model_inputs, return_dict=True)
   3208 # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping
   3209 model_kwargs = self._update_model_kwargs_for_generation(
   3210     outputs,
   3211     model_kwargs,
   3212     is_encoder_decoder=self.config.is_encoder_decoder,
   3213 )

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)
   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1738 else:
-> 1739     return self._call_impl(*args, **kwargs)

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)
   1745 # If we don't have any hooks, we want to skip the rest of the logic in
   1746 # this function, and just call forward.
   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1748         or _global_backward_pre_hooks or _global_backward_hooks
   1749         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1750     return forward_call(*args, **kwargs)
   1752 result = None
   1753 called_always_called_hooks = set()

File ~/.cache/huggingface/modules/transformers_modules/deepseek-ocr/modeling_deepseekocr.py:565, in DeepseekOCRForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, images_seq_mask, images_spatial_crop, return_dict)
    558 output_hidden_states = (
    559     output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    560 )
    561 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
--> 565 outputs  = self.model(
    566     input_ids=input_ids,
    567     past_key_values=past_key_values,
    568     attention_mask=attention_mask,
    569     position_ids=position_ids,
    570     inputs_embeds=inputs_embeds,
    571     use_cache=use_cache,
    572     output_attentions=output_attentions,
    573     output_hidden_states=output_hidden_states,
    574     images=images,
    575     images_seq_mask = images_seq_mask,
    576     images_spatial_crop = images_spatial_crop,
    577     return_dict=return_dict
    578     
    579 )
    583 # print(transformer_outputs)
    585 hidden_states = outputs[0]

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)
   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1738 else:
-> 1739     return self._call_impl(*args, **kwargs)

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)
   1745 # If we don't have any hooks, we want to skip the rest of the logic in
   1746 # this function, and just call forward.
   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1748         or _global_backward_pre_hooks or _global_backward_hooks
   1749         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1750     return forward_call(*args, **kwargs)
   1752 result = None
   1753 called_always_called_hooks = set()

File ~/.cache/huggingface/modules/transformers_modules/deepseek-ocr/modeling_deepseekocr.py:510, in DeepseekOCRModel.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, images, images_seq_mask, images_spatial_crop, return_dict)
    505             inputs_embeds[idx].masked_scatter_(images_seq_mask[idx].unsqueeze(-1).cuda(), images_in_this_batch)
    507         idx += 1
--> 510 return super(DeepseekOCRModel, self).forward(
    511     input_ids=None, attention_mask=attention_mask, past_key_values=past_key_values,
    512     inputs_embeds=inputs_embeds, use_cache=use_cache, position_ids = position_ids,
    513     output_attentions=output_attentions, output_hidden_states=output_hidden_states,
    514     return_dict=return_dict
    515 )

File ~/.cache/huggingface/modules/transformers_modules/deepseek-ocr/modeling_deepseekv2.py:1596, in DeepseekV2Model.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)
   1586     layer_outputs = self._gradient_checkpointing_func(
   1587         decoder_layer.__call__,
   1588         hidden_states,
   (...)
   1593         use_cache,
   1594     )
   1595 else:
-> 1596     layer_outputs = decoder_layer(
   1597         hidden_states,
   1598         attention_mask=attention_mask,
   1599         position_ids=position_ids,
   1600         past_key_value=past_key_values,
   1601         output_attentions=output_attentions,
   1602         use_cache=use_cache,
   1603     )
   1605 hidden_states = layer_outputs[0]
   1607 if use_cache:

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)
   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1738 else:
-> 1739     return self._call_impl(*args, **kwargs)

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)
   1745 # If we don't have any hooks, we want to skip the rest of the logic in
   1746 # this function, and just call forward.
   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1748         or _global_backward_pre_hooks or _global_backward_hooks
   1749         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1750     return forward_call(*args, **kwargs)
   1752 result = None
   1753 called_always_called_hooks = set()

File ~/.cache/huggingface/modules/transformers_modules/deepseek-ocr/modeling_deepseekv2.py:1308, in DeepseekV2DecoderLayer.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)
   1305 hidden_states = self.input_layernorm(hidden_states)
   1307 # Self Attention
-> 1308 hidden_states, self_attn_weights, present_key_value = self.self_attn(
   1309     hidden_states=hidden_states,
   1310     attention_mask=attention_mask,
   1311     position_ids=position_ids,
   1312     past_key_value=past_key_value,
   1313     output_attentions=output_attentions,
   1314     use_cache=use_cache,
   1315     **kwargs,
   1316 )
   1317 hidden_states = residual + hidden_states
   1319 # Fully Connected

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)
   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1738 else:
-> 1739     return self._call_impl(*args, **kwargs)

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)
   1745 # If we don't have any hooks, we want to skip the rest of the logic in
   1746 # this function, and just call forward.
   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1748         or _global_backward_pre_hooks or _global_backward_hooks
   1749         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1750     return forward_call(*args, **kwargs)
   1752 result = None
   1753 called_always_called_hooks = set()

File /opt/micromamba/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:498, in LlamaFlashAttention2.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)
    495     key_states = key_states.to(target_dtype)
    496     value_states = value_states.to(target_dtype)
--> 498 attn_output = _flash_attention_forward(
    499     query_states,
    500     key_states,
    501     value_states,
    502     attention_mask,
    503     q_len,
    504     position_ids=position_ids,
    505     dropout=dropout_rate,
    506     sliding_window=getattr(self, "sliding_window", None),
    507     use_top_left_mask=self._flash_attn_uses_top_left_mask,
    508     is_causal=self.is_causal,
    509 )
    511 attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
    512 attn_output = self.o_proj(attn_output)

File /opt/micromamba/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py:297, in _flash_attention_forward(query_states, key_states, value_states, attention_mask, query_length, is_causal, dropout, position_ids, softmax_scale, sliding_window, use_top_left_mask, softcap, deterministic)
    294     attn_output = attn_output.view(batch_size, -1, attn_output.size(-2), attn_output.size(-1))
    296 else:
--> 297     attn_output = flash_attn_func(
    298         query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal, **flash_kwargs
    299     )
    301 return attn_output

File /opt/micromamba/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:1201, in flash_attn_func(q, k, v, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_attn_probs)
   1140 def flash_attn_func(
   1141     q,
   1142     k,
   (...)
   1151     return_attn_probs=False,
   1152 ):
   1153     """dropout_p should be set to 0.0 during evaluation
   1154     Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads
   1155     than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.
   (...)
   1199             pattern (negative means that location was dropped, nonnegative means it was kept).
   1200     """
-> 1201     return FlashAttnFunc.apply(
   1202         q,
   1203         k,
   1204         v,
   1205         dropout_p,
   1206         softmax_scale,
   1207         causal,
   1208         window_size,
   1209         softcap,
   1210         alibi_slopes,
   1211         deterministic,
   1212         return_attn_probs,
   1213         torch.is_grad_enabled(),
   1214     )

File /opt/micromamba/lib/python3.10/site-packages/torch/autograd/function.py:575, in Function.apply(cls, *args, **kwargs)
    572 if not torch._C._are_functorch_transforms_active():
    573     # See NOTE: [functorch vjp and autograd interaction]
    574     args = _functorch.utils.unwrap_dead_wrappers(args)
--> 575     return super().apply(*args, **kwargs)  # type: ignore[misc]
    577 if not is_setup_ctx_defined:
    578     raise RuntimeError(
    579         "In order to use an autograd.Function with functorch transforms "
    580         "(vmap, grad, jvp, jacrev, ...), it must override the setup_context "
    581         "staticmethod. For more details, please see "
    582         "https://pytorch.org/docs/main/notes/extending.func.html"
    583     )

File /opt/micromamba/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:839, in FlashAttnFunc.forward(ctx, q, k, v, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_softmax, is_grad_enabled)
    837     k = torch.nn.functional.pad(k, [0, 8 - head_size_og % 8])
    838     v = torch.nn.functional.pad(v, [0, 8 - head_size_og % 8])
--> 839 out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_forward(
    840     q,
    841     k,
    842     v,
    843     dropout_p,
    844     softmax_scale,
    845     causal=causal,
    846     window_size_left=window_size[0],
    847     window_size_right=window_size[1],
    848     softcap=softcap,
    849     alibi_slopes=alibi_slopes,
    850     return_softmax=return_softmax and dropout_p > 0,
    851 )
    852 if is_grad:
    853     ctx.save_for_backward(q, k, v, out_padded, softmax_lse, rng_state)

File /opt/micromamba/lib/python3.10/site-packages/torch/_ops.py:1123, in OpOverloadPacket.__call__(self, *args, **kwargs)
   1121 if self._has_torchbind_op_overload and _must_dispatch_in_python(args, kwargs):
   1122     return _call_overload_packet_from_python(self, args, kwargs)
-> 1123 return self._op(*args, **(kwargs or {}))

File /opt/micromamba/lib/python3.10/site-packages/torch/_library/autograd.py:113, in make_autograd_impl.<locals>.autograd_impl(keyset, *args, **keyword_only_args)
    111     result = Generated.apply(*args, Metadata(keyset, keyword_only_args))  # type: ignore[attr-defined]
    112 else:
--> 113     result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
    114 return result

File /opt/micromamba/lib/python3.10/site-packages/torch/_library/autograd.py:40, in make_autograd_impl.<locals>.forward_no_grad(*args)
     38 keyset = metadata.keyset
     39 kwargs = metadata.keyword_only_args
---> 40 result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
     41 return result

File /opt/micromamba/lib/python3.10/site-packages/torch/_ops.py:728, in OpOverload.redispatch(self, keyset, *args, **kwargs)
    727 def redispatch(self, /, keyset, *args, **kwargs):
--> 728     return self._handle.redispatch_boxed(keyset, *args, **kwargs)

File /opt/micromamba/lib/python3.10/site-packages/torch/_library/custom_ops.py:305, in CustomOpDef.register_kernel.<locals>.inner.<locals>.backend_impl(*args, **kwargs)
    304 def backend_impl(*args, **kwargs):
--> 305     result = self._backend_fns[device_type](*args, **kwargs)
    307     def get_module():
    308         fn = self._backend_fns[device_type]

File /opt/micromamba/lib/python3.10/site-packages/torch/_compile.py:32, in _disable_dynamo.<locals>.inner(*args, **kwargs)
     29     disable_fn = torch._dynamo.disable(fn, recursive)
     30     fn.__dynamo_disable = disable_fn
---> 32 return disable_fn(*args, **kwargs)

File /opt/micromamba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745, in DisableContext.__call__.<locals>._fn(*args, **kwargs)
    741 prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(
    742     _is_skip_guard_eval_unsafe_stance()
    743 )
    744 try:
--> 745     return fn(*args, **kwargs)
    746 finally:
    747     _maybe_set_eval_frame(prior)

File /opt/micromamba/lib/python3.10/site-packages/torch/_library/custom_ops.py:337, in CustomOpDef.register_kernel.<locals>.inner.<locals>.wrapped_fn(*args, **kwargs)
    335     return self._init_fn(*args, **kwargs)
    336 else:
--> 337     return fn(*args, **kwargs)

File /opt/micromamba/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:96, in _flash_attn_forward(q, k, v, dropout_p, softmax_scale, causal, window_size_left, window_size_right, softcap, alibi_slopes, return_softmax)
     81 @_torch_custom_op_wrapper("flash_attn::_flash_attn_forward", mutates_args=(), device_types="cuda")
     82 def _flash_attn_forward(
     83     q: torch.Tensor,
   (...)
     93     return_softmax: bool
     94 ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
     95     q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
---> 96     out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(
     97         q,
     98         k,
     99         v,
    100         None,
    101         alibi_slopes,
    102         dropout_p,
    103         softmax_scale,
    104         causal,
    105         window_size_left,
    106         window_size_right,
    107         softcap,
    108         return_softmax,
    109         None,
    110     )
    111     return out, softmax_lse, S_dmask, rng_state

RuntimeError: FlashAttention only supports Ampere GPUs or newer.
```

I run main.py in a vertex instance which is built using the Dockerfile attached
this docker file has some packages installed 
when I run main.py, it throws the error above

solve the error
