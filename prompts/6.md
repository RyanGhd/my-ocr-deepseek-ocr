error
```
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /opt/models/deepseek-ocr and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[12], line 33
     22 # Use 'eager' attention instead of 'flash_attention_2' for non-Ampere GPUs (T4, V100, etc.)
     23 # Options: 'eager' (standard), 'sdpa' (scaled dot product), 'flash_attention_2' (Ampere+ only)
     24 model = AutoModel.from_pretrained(
     25     model_path, 
     26     attn_implementation='eager', 
   (...)
     30     torch_dtype=torch.bfloat16
     31 )
---> 33 model = model.eval().cuda().to(torch.bfloat16)
     35 prompt = "<image>\n<|grounding|>Convert the document to markdown. "
     36 image_file = 'images/5.png'

File /opt/micromamba/lib/python3.10/site-packages/transformers/modeling_utils.py:3117, in PreTrainedModel.cuda(self, *args, **kwargs)
   3112         raise ValueError(
   3113             "Calling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. "
   3114             f"The current device is `{self.device}`. If you intended to move the model, please install bitsandbytes >= 0.43.2."
   3115         )
   3116 else:
-> 3117     return super().cuda(*args, **kwargs)

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1053, in Module.cuda(self, device)
   1036 def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:
   1037     r"""Move all model parameters and buffers to the GPU.
   1038 
   1039     This also makes associated parameters and buffers different objects. So
   (...)
   1051         Module: self
   1052     """
-> 1053     return self._apply(lambda t: t.cuda(device))

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:903, in Module._apply(self, fn, recurse)
    901 if recurse:
    902     for module in self.children():
--> 903         module._apply(fn)
    905 def compute_should_use_set_data(tensor, tensor_applied):
    906     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
    907         # If the new tensor has compatible tensor type as the existing tensor,
    908         # the current behavior is to change the tensor in-place using `.data =`,
   (...)
    913         # global flag to let the user control whether they want the future
    914         # behavior of overwriting the existing tensor or not.

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:903, in Module._apply(self, fn, recurse)
    901 if recurse:
    902     for module in self.children():
--> 903         module._apply(fn)
    905 def compute_should_use_set_data(tensor, tensor_applied):
    906     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
    907         # If the new tensor has compatible tensor type as the existing tensor,
    908         # the current behavior is to change the tensor in-place using `.data =`,
   (...)
    913         # global flag to let the user control whether they want the future
    914         # behavior of overwriting the existing tensor or not.

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:930, in Module._apply(self, fn, recurse)
    926 # Tensors stored in modules are graph leaves, and we don't want to
    927 # track autograd history of `param_applied`, so we have to use
    928 # `with torch.no_grad():`
    929 with torch.no_grad():
--> 930     param_applied = fn(param)
    931 p_should_use_set_data = compute_should_use_set_data(param, param_applied)
    933 # subclasses may have multiple child tensors so we need to use swap_tensors

File /opt/micromamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1053, in Module.cuda.<locals>.<lambda>(t)
   1036 def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:
   1037     r"""Move all model parameters and buffers to the GPU.
   1038 
   1039     This also makes associated parameters and buffers different objects. So
   (...)
   1051         Module: self
   1052     """
-> 1053     return self._apply(lambda t: t.cuda(device))

RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

```

I run main_high_mem_with_crop.py in a vertex instance which is built using the Dockerfile attached
this docker file has some packages installed 
 main_high_mem_with_crop.py used to work fine but all of sudden started to throw the error above

solve the error
